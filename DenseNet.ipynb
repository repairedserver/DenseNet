{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8HF89xmcDnDr/pdm4homx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/repairedserver/DenseNet/blob/main/DenseNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "w5rBzKZmqXnp"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=64\n",
        "learning_rate = 0.1\n",
        "layers = 100\n",
        "\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('../data',train=True,download=True,transform=transform_train),\n",
        "    batch_size=batch_size,shuffle=True\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('../data',train=False,transform=transform_test),\n",
        "    batch_size=batch_size,shuffle=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzdXlx16qb8o",
        "outputId": "480958a1-b292-4e08-c425-c0f77181d353"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseBlock(nn.Module):\n",
        "    def __init__(self,nb_layers,in_planes,growh_rate,block,dropRate=0.0):\n",
        "        super(DenseBlock,self).__init__()\n",
        "        self.layer = self._make_layer(block, in_planes, growh_rate, nb_layers, dropRate)\n",
        "    \n",
        "    def _make_layer(self,block,in_planes,growh_rate,nb_layers,dropRate):\n",
        "        layers=[]\n",
        "        for i in range(nb_layers):\n",
        "            layers.append(block(in_planes + i*growh_rate ,growh_rate,dropRate))\n",
        "            \n",
        "        return nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self,x):\n",
        "        return self.layer(x)"
      ],
      "metadata": {
        "id": "efXcdPXDqiIt"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self,in_planes,out_planes,dropRate = 0.0):\n",
        "        #input dimsnsion을 정하고, output dimension을 정하고(growh_rate임), dropRate를 정함.\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.relu = nn.ReLU(inplace = True) # inplace 하면 input으로 들어온 것 자체를 수정하겠다는 뜻. 메모리 usage가 좀 좋아짐. 하지만 input을 없앰.\n",
        "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride = 1, padding = 1, bias = False)\n",
        "        self.droprate = dropRate\n",
        "        \n",
        "    def forward(self,x):\n",
        "        out = self.conv1(self.relu(self.bn1(x)))\n",
        "        if self.droprate>0:\n",
        "            out = F.dropout (out,p=self.droprate,training = self.training)\n",
        "        return torch.cat([x,out],1)\n",
        "        \n",
        "class BottleneckBlock(nn.Module):\n",
        "    def __init__(self,in_planes,out_planes,dropRate=0.0):\n",
        "        #out_planes => growh_rate를 입력으로 받게 된다.\n",
        "        super(BottleneckBlock,self).__init__()\n",
        "        inter_planes = out_planes * 4 # bottleneck layer의 conv 1x1 filter chennel 수는 4*growh_rate이다.\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.relu = nn.ReLU(inplace = True)\n",
        "        self.conv1 = nn.Conv2d(in_planes,inter_planes,kernel_size=1,stride=1,padding=0,bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(inter_planes)\n",
        "        self.conv2 = nn.Conv2d(inter_planes,out_planes,kernel_size=3,stride=1,padding=1,bias=False)\n",
        "        self.droprate = dropRate\n",
        "        \n",
        "    def forward(self,x):\n",
        "        out = self.conv1(self.relu(self.bn1(x)))\n",
        "        if self.droprate>0:\n",
        "            out = F.dropout(out,p=self.droprate,inplace=False,training = self.training)\n",
        "        out = self.conv2(self.relu(self.bn2(out)))\n",
        "        if self.droprate>0:\n",
        "            out = F.dropout(out,p=self.droprate,inplace=False,training = self.training)\n",
        "        return torch.cat([x,out],1)"
      ],
      "metadata": {
        "id": "V6cyTqRuq57m"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransitionBlock(nn.Module):\n",
        "    def __init__(self,in_planes,out_planes,dropRate=0.0):\n",
        "        super(TransitionBlock,self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_planes,out_planes,kernel_size=1,stride=1,padding=0,bias=False)\n",
        "        self.droprate = dropRate\n",
        "        \n",
        "    def forward(self,x):\n",
        "        out = self.conv1(self.relu(self.bn1(x)))\n",
        "        if self.droprate>0:\n",
        "            out = F.dropout(out,p=self.droprate,inplace=False,training=self.training)\n",
        "        return F.avg_pool2d(out,2)"
      ],
      "metadata": {
        "id": "1k8mwRSlqrvn"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseNet(nn.Module):\n",
        "    def __init__(self,depth,num_classes,growh_rate=12,reduction=0.5,bottleneck=True,dropRate=0.0):\n",
        "        super(DenseNet,self).__init__()\n",
        "        num_of_blocks = 3\n",
        "        in_planes = 16 \n",
        "        n = (depth - num_of_blocks - 1)/num_of_blocks # 총 depth에서 첫 conv , 2개의 transit , 마지막 linear 빼고 / num_of_blocks\n",
        "        if reduction != 1 :\n",
        "            in_planes = 2 * growh_rate\n",
        "        if bottleneck == True:\n",
        "            in_planes = 2 * growh_rate #논문에서 Bottleneck + Compression 할 경우 first layer은 2*growh_rate라고 했다.\n",
        "            n = n/2 # conv 1x1 레이어가 추가되니까 !\n",
        "            block = BottleneckBlock \n",
        "        else :\n",
        "            block = BasicBlock\n",
        "        \n",
        "        n = int(n) #n = DenseBlock에서 block layer 개수를 의미한다.\n",
        "        self.conv1 = nn.Conv2d(3,in_planes,kernel_size=3,stride=1,padding=1,bias=False) # input:RGB -> output:growhR*2\n",
        "        \n",
        "        \n",
        "        #1st block\n",
        "        # nb_layers,in_planes,growh_rate,block,dropRate\n",
        "        self.block1 = DenseBlock(n,in_planes,growh_rate,block,dropRate)\n",
        "        in_planes = int(in_planes+n*growh_rate) # 입력 + 레이어 만큼의 growh_rate\n",
        "        \n",
        "        # in_planes,out_planes,dropRate\n",
        "        self.trans1 = TransitionBlock(in_planes, int(math.floor(in_planes*reduction)),dropRate=dropRate)\n",
        "        in_planes = int(math.floor(in_planes*reduction))\n",
        "        \n",
        "        \n",
        "        #2nd block\n",
        "        # nb_layers,in_planes,growh_rate,block,dropRate\n",
        "        self.block2 = DenseBlock(n,in_planes,growh_rate,block,dropRate)\n",
        "        in_planes = int(in_planes+n*growh_rate) # 입력 + 레이어 만큼의 growh_rate\n",
        "        \n",
        "        # in_planes,out_planes,dropRate\n",
        "        self.trans2 = TransitionBlock(in_planes, int(math.floor(in_planes*reduction)),dropRate=dropRate)\n",
        "        in_planes = int(math.floor(in_planes*reduction))\n",
        "        \n",
        "        \n",
        "        #3rd block\n",
        "        # nb_layers,in_planes,growh_rate,block,dropRate\n",
        "        self.block3 = DenseBlock(n,in_planes,growh_rate,block,dropRate)\n",
        "        in_planes = int(in_planes+n*growh_rate) # 입력 + 레이어 만큼의 growh_rate\n",
        "        \n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.relu = nn.ReLU(inplace = True)\n",
        "        \n",
        "        self.fc = nn.Linear(in_planes,num_classes) # 마지막에 ave_pool 후에 1x1 size의 결과만 남음.\n",
        "        \n",
        "        self.in_planes = in_planes\n",
        "        \n",
        "        # module 초기화\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                # Conv layer들은 필터에서 나오는 분산 root(2/n)로 normalize 함\n",
        "                # mean = 0 , 분산 = sqrt(2/n) // 이게 무슨 초기화 방법이었는지 기억이 안난다.\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d): # shifting param이랑 scaling param 초기화(?)\n",
        "                m.weight.data.fill_(1) # \n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):# linear layer 초기화.\n",
        "                m.bias.data.zero_()\n",
        "        \n",
        "    def forward(self,x):\n",
        "        #x : 32*32\n",
        "        out = self.conv1(x) # 32*32\n",
        "        out = self.block1(out) # 32*32\n",
        "        out = self.trans1(out) # 16*16\n",
        "        out = self.block2(out) # 16*16\n",
        "        out = self.trans2(out) # 8*8\n",
        "        out = self.block3(out) # 8*8\n",
        "        out = self.relu(self.bn1(out)) #8*8\n",
        "        out = F.avg_pool2d(out,8) #1*1\n",
        "        out = out.view(-1, self.in_planes) #channel수만 남기 때문에 Linear -> in_planes\n",
        "        return self.fc(out)"
      ],
      "metadata": {
        "id": "a0SJa3Mcquge"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DenseNet(layers,10,growh_rate=12,dropRate = 0.0)\n",
        "\n",
        "print('Number of model parameters: {}'.format(\n",
        "    sum([p.data.nelement() for p in model.parameters()])))\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate,\n",
        "                            momentum=0.9,nesterov=True,weight_decay=1e-4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6h_Dahaq9Vo",
        "outputId": "5f5e33bb-22f3-4792-8cca-68ee00e636fc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of model parameters: 769162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_loader,model,criterion,optimizer,epoch):\n",
        "    model.train()\n",
        "    for i, (input,target) in enumerate(train_loader):\n",
        "        target = target.to(device)\n",
        "        input = input.to(device)\n",
        "        \n",
        "        output = model(input)\n",
        "        loss = criterion(output,target)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if(i%20 == 0):\n",
        "            print(\"loss in epoch %d , step %d : %f\" % (epoch, i, loss.data))"
      ],
      "metadata": {
        "id": "kGZoWYZKrEJS"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(test_loader,model,criterion,epoch):\n",
        "    model.eval()\n",
        "    \n",
        "    correct = 0\n",
        "    \n",
        "    \n",
        "    for i, (input,target) in enumerate(test_loader):\n",
        "        target = target.to(device)\n",
        "        input = input.to(device)\n",
        "        \n",
        "        output = model(input)\n",
        "        loss = criterion(output,target)\n",
        "        \n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().float().sum()\n",
        "    \n",
        "    print(\"Accuracy in epoch %d : %f\" % (epoch,100.0*correct/len(test_loader.dataset)))"
      ],
      "metadata": {
        "id": "OPJ6acFsrHoc"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_lr(optimizer, epoch, learning_rate):\n",
        "    if epoch==150 :\n",
        "        learning_rate*=0.1\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = learning_rate"
      ],
      "metadata": {
        "id": "FArdmjeirJ7c"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(0, 30):\n",
        "    adjust_lr(optimizer,epoch,learning_rate)\n",
        "    train(train_loader,model,criterion,optimizer,epoch)\n",
        "    test(test_loader,model,criterion,epoch)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hTHAZvYrM4v",
        "outputId": "91759e05-1c5b-43cd-a058-eadb7629f669"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0 , step 0 : 2.341634\n",
            "loss in epoch 0 , step 20 : 2.077102\n",
            "loss in epoch 0 , step 40 : 2.201866\n",
            "loss in epoch 0 , step 60 : 1.920092\n",
            "loss in epoch 0 , step 80 : 1.920340\n",
            "loss in epoch 0 , step 100 : 1.795943\n",
            "loss in epoch 0 , step 120 : 1.787989\n",
            "loss in epoch 0 , step 140 : 1.430708\n",
            "loss in epoch 0 , step 160 : 2.015533\n",
            "loss in epoch 0 , step 180 : 1.726802\n",
            "loss in epoch 0 , step 200 : 1.657656\n",
            "loss in epoch 0 , step 220 : 1.482993\n",
            "loss in epoch 0 , step 240 : 1.475048\n",
            "loss in epoch 0 , step 260 : 1.512459\n",
            "loss in epoch 0 , step 280 : 1.364055\n",
            "loss in epoch 0 , step 300 : 1.406588\n",
            "loss in epoch 0 , step 320 : 1.424609\n",
            "loss in epoch 0 , step 340 : 1.570875\n",
            "loss in epoch 0 , step 360 : 1.624816\n",
            "loss in epoch 0 , step 380 : 1.383822\n",
            "loss in epoch 0 , step 400 : 0.987856\n",
            "loss in epoch 0 , step 420 : 1.306162\n",
            "loss in epoch 0 , step 440 : 1.384524\n",
            "loss in epoch 0 , step 460 : 1.253651\n",
            "loss in epoch 0 , step 480 : 1.141056\n",
            "loss in epoch 0 , step 500 : 1.157507\n",
            "loss in epoch 0 , step 520 : 1.164079\n",
            "loss in epoch 0 , step 540 : 1.356666\n",
            "loss in epoch 0 , step 560 : 1.198187\n",
            "loss in epoch 0 , step 580 : 1.211018\n",
            "loss in epoch 0 , step 600 : 1.109179\n",
            "loss in epoch 0 , step 620 : 1.061666\n",
            "loss in epoch 0 , step 640 : 1.093607\n",
            "loss in epoch 0 , step 660 : 1.128316\n",
            "loss in epoch 0 , step 680 : 1.116638\n",
            "loss in epoch 0 , step 700 : 1.044665\n",
            "loss in epoch 0 , step 720 : 1.143323\n",
            "loss in epoch 0 , step 740 : 0.887651\n",
            "loss in epoch 0 , step 760 : 1.016189\n",
            "loss in epoch 0 , step 780 : 1.293172\n",
            "Accuracy in epoch 0 : 60.980000\n",
            "loss in epoch 1 , step 0 : 0.968642\n",
            "loss in epoch 1 , step 20 : 0.993702\n",
            "loss in epoch 1 , step 40 : 0.731552\n",
            "loss in epoch 1 , step 60 : 1.013043\n",
            "loss in epoch 1 , step 80 : 1.123376\n",
            "loss in epoch 1 , step 100 : 0.854063\n",
            "loss in epoch 1 , step 120 : 0.942831\n",
            "loss in epoch 1 , step 140 : 0.858106\n",
            "loss in epoch 1 , step 160 : 0.985349\n",
            "loss in epoch 1 , step 180 : 0.709768\n",
            "loss in epoch 1 , step 200 : 0.977287\n",
            "loss in epoch 1 , step 220 : 0.719156\n",
            "loss in epoch 1 , step 240 : 0.837068\n",
            "loss in epoch 1 , step 260 : 0.985115\n",
            "loss in epoch 1 , step 280 : 1.123137\n",
            "loss in epoch 1 , step 300 : 0.876868\n",
            "loss in epoch 1 , step 320 : 1.170887\n",
            "loss in epoch 1 , step 340 : 0.689480\n",
            "loss in epoch 1 , step 360 : 0.880366\n",
            "loss in epoch 1 , step 380 : 0.864415\n",
            "loss in epoch 1 , step 400 : 0.733721\n",
            "loss in epoch 1 , step 420 : 1.169513\n",
            "loss in epoch 1 , step 440 : 0.932452\n",
            "loss in epoch 1 , step 460 : 0.947921\n",
            "loss in epoch 1 , step 480 : 1.112056\n",
            "loss in epoch 1 , step 500 : 0.795718\n",
            "loss in epoch 1 , step 520 : 0.636468\n",
            "loss in epoch 1 , step 540 : 0.864478\n",
            "loss in epoch 1 , step 560 : 0.755818\n",
            "loss in epoch 1 , step 580 : 0.852461\n",
            "loss in epoch 1 , step 600 : 0.689277\n",
            "loss in epoch 1 , step 620 : 0.626606\n",
            "loss in epoch 1 , step 640 : 1.004704\n",
            "loss in epoch 1 , step 660 : 0.633777\n",
            "loss in epoch 1 , step 680 : 0.545787\n",
            "loss in epoch 1 , step 700 : 0.733077\n",
            "loss in epoch 1 , step 720 : 0.763333\n",
            "loss in epoch 1 , step 740 : 0.821521\n",
            "loss in epoch 1 , step 760 : 0.921592\n",
            "loss in epoch 1 , step 780 : 0.830341\n",
            "Accuracy in epoch 1 : 67.900002\n",
            "loss in epoch 2 , step 0 : 0.665163\n",
            "loss in epoch 2 , step 20 : 0.739119\n",
            "loss in epoch 2 , step 40 : 0.795557\n",
            "loss in epoch 2 , step 60 : 0.726187\n",
            "loss in epoch 2 , step 80 : 0.716209\n",
            "loss in epoch 2 , step 100 : 0.704159\n",
            "loss in epoch 2 , step 120 : 0.843880\n",
            "loss in epoch 2 , step 140 : 0.591644\n",
            "loss in epoch 2 , step 160 : 0.758246\n",
            "loss in epoch 2 , step 180 : 0.879021\n",
            "loss in epoch 2 , step 200 : 0.914186\n",
            "loss in epoch 2 , step 220 : 0.474899\n",
            "loss in epoch 2 , step 240 : 0.567854\n",
            "loss in epoch 2 , step 260 : 1.103136\n",
            "loss in epoch 2 , step 280 : 0.539465\n",
            "loss in epoch 2 , step 300 : 0.539774\n",
            "loss in epoch 2 , step 320 : 0.612796\n",
            "loss in epoch 2 , step 340 : 0.816501\n",
            "loss in epoch 2 , step 360 : 0.615620\n",
            "loss in epoch 2 , step 380 : 0.650885\n",
            "loss in epoch 2 , step 400 : 0.709065\n",
            "loss in epoch 2 , step 420 : 0.652451\n",
            "loss in epoch 2 , step 440 : 0.762190\n",
            "loss in epoch 2 , step 460 : 0.929361\n",
            "loss in epoch 2 , step 480 : 0.834890\n",
            "loss in epoch 2 , step 500 : 0.790529\n",
            "loss in epoch 2 , step 520 : 0.663695\n",
            "loss in epoch 2 , step 540 : 0.676542\n",
            "loss in epoch 2 , step 560 : 0.669163\n",
            "loss in epoch 2 , step 580 : 0.871501\n",
            "loss in epoch 2 , step 600 : 0.780968\n",
            "loss in epoch 2 , step 620 : 0.722693\n",
            "loss in epoch 2 , step 640 : 0.570553\n",
            "loss in epoch 2 , step 660 : 0.816672\n",
            "loss in epoch 2 , step 680 : 0.493377\n",
            "loss in epoch 2 , step 700 : 0.640852\n",
            "loss in epoch 2 , step 720 : 0.746582\n",
            "loss in epoch 2 , step 740 : 0.586802\n",
            "loss in epoch 2 , step 760 : 0.562519\n",
            "loss in epoch 2 , step 780 : 0.676664\n",
            "Accuracy in epoch 2 : 70.019997\n",
            "loss in epoch 3 , step 0 : 0.586891\n",
            "loss in epoch 3 , step 20 : 0.677061\n",
            "loss in epoch 3 , step 40 : 0.743273\n",
            "loss in epoch 3 , step 60 : 0.437228\n",
            "loss in epoch 3 , step 80 : 0.513785\n",
            "loss in epoch 3 , step 100 : 0.577992\n",
            "loss in epoch 3 , step 120 : 0.715044\n",
            "loss in epoch 3 , step 140 : 0.730263\n",
            "loss in epoch 3 , step 160 : 0.684561\n",
            "loss in epoch 3 , step 180 : 0.643846\n",
            "loss in epoch 3 , step 200 : 0.600293\n",
            "loss in epoch 3 , step 220 : 0.473272\n",
            "loss in epoch 3 , step 240 : 0.697588\n",
            "loss in epoch 3 , step 260 : 0.541252\n",
            "loss in epoch 3 , step 280 : 0.683918\n",
            "loss in epoch 3 , step 300 : 0.860957\n",
            "loss in epoch 3 , step 320 : 0.684259\n",
            "loss in epoch 3 , step 340 : 0.562087\n",
            "loss in epoch 3 , step 360 : 0.562505\n",
            "loss in epoch 3 , step 380 : 0.520195\n",
            "loss in epoch 3 , step 400 : 0.525009\n",
            "loss in epoch 3 , step 420 : 0.654276\n",
            "loss in epoch 3 , step 440 : 0.559946\n",
            "loss in epoch 3 , step 460 : 0.513725\n",
            "loss in epoch 3 , step 480 : 0.701835\n",
            "loss in epoch 3 , step 500 : 0.388364\n",
            "loss in epoch 3 , step 520 : 0.416083\n",
            "loss in epoch 3 , step 540 : 0.357860\n",
            "loss in epoch 3 , step 560 : 0.445526\n",
            "loss in epoch 3 , step 580 : 0.432186\n",
            "loss in epoch 3 , step 600 : 0.628438\n",
            "loss in epoch 3 , step 620 : 0.481825\n",
            "loss in epoch 3 , step 640 : 0.751171\n",
            "loss in epoch 3 , step 660 : 0.615183\n",
            "loss in epoch 3 , step 680 : 0.491939\n",
            "loss in epoch 3 , step 700 : 0.589980\n",
            "loss in epoch 3 , step 720 : 0.703024\n",
            "loss in epoch 3 , step 740 : 0.570279\n",
            "loss in epoch 3 , step 760 : 0.629600\n",
            "loss in epoch 3 , step 780 : 0.655780\n",
            "Accuracy in epoch 3 : 73.169998\n",
            "loss in epoch 4 , step 0 : 0.578941\n",
            "loss in epoch 4 , step 20 : 0.684005\n",
            "loss in epoch 4 , step 40 : 0.551285\n",
            "loss in epoch 4 , step 60 : 0.691542\n",
            "loss in epoch 4 , step 80 : 0.559797\n",
            "loss in epoch 4 , step 100 : 0.404752\n",
            "loss in epoch 4 , step 120 : 0.478620\n",
            "loss in epoch 4 , step 140 : 0.409125\n",
            "loss in epoch 4 , step 160 : 0.734138\n",
            "loss in epoch 4 , step 180 : 0.453726\n",
            "loss in epoch 4 , step 200 : 0.632404\n",
            "loss in epoch 4 , step 220 : 0.602335\n",
            "loss in epoch 4 , step 240 : 0.435782\n",
            "loss in epoch 4 , step 260 : 0.559269\n",
            "loss in epoch 4 , step 280 : 0.651413\n",
            "loss in epoch 4 , step 300 : 0.344879\n",
            "loss in epoch 4 , step 320 : 0.460696\n",
            "loss in epoch 4 , step 340 : 0.465330\n",
            "loss in epoch 4 , step 360 : 0.370620\n",
            "loss in epoch 4 , step 380 : 0.426905\n",
            "loss in epoch 4 , step 400 : 0.627353\n",
            "loss in epoch 4 , step 420 : 0.777396\n",
            "loss in epoch 4 , step 440 : 0.749178\n",
            "loss in epoch 4 , step 460 : 0.617278\n",
            "loss in epoch 4 , step 480 : 0.365538\n",
            "loss in epoch 4 , step 500 : 0.588025\n",
            "loss in epoch 4 , step 520 : 0.739035\n",
            "loss in epoch 4 , step 540 : 0.498112\n",
            "loss in epoch 4 , step 560 : 0.469976\n",
            "loss in epoch 4 , step 580 : 0.381693\n",
            "loss in epoch 4 , step 600 : 0.582390\n",
            "loss in epoch 4 , step 620 : 0.382299\n",
            "loss in epoch 4 , step 640 : 0.568083\n",
            "loss in epoch 4 , step 660 : 0.544078\n",
            "loss in epoch 4 , step 680 : 0.556910\n",
            "loss in epoch 4 , step 700 : 0.394684\n",
            "loss in epoch 4 , step 720 : 0.543349\n",
            "loss in epoch 4 , step 740 : 0.378820\n",
            "loss in epoch 4 , step 760 : 0.405837\n",
            "loss in epoch 4 , step 780 : 0.378750\n",
            "Accuracy in epoch 4 : 81.699997\n",
            "loss in epoch 5 , step 0 : 0.492597\n",
            "loss in epoch 5 , step 20 : 0.330072\n",
            "loss in epoch 5 , step 40 : 0.761408\n",
            "loss in epoch 5 , step 60 : 0.414898\n",
            "loss in epoch 5 , step 80 : 0.465070\n",
            "loss in epoch 5 , step 100 : 0.436141\n",
            "loss in epoch 5 , step 120 : 0.341139\n",
            "loss in epoch 5 , step 140 : 0.344250\n",
            "loss in epoch 5 , step 160 : 0.429236\n",
            "loss in epoch 5 , step 180 : 0.256371\n",
            "loss in epoch 5 , step 200 : 0.577663\n",
            "loss in epoch 5 , step 220 : 0.382522\n",
            "loss in epoch 5 , step 240 : 0.486538\n",
            "loss in epoch 5 , step 260 : 0.504884\n",
            "loss in epoch 5 , step 280 : 0.450922\n",
            "loss in epoch 5 , step 300 : 0.435664\n",
            "loss in epoch 5 , step 320 : 0.567952\n",
            "loss in epoch 5 , step 340 : 0.587547\n",
            "loss in epoch 5 , step 360 : 0.516274\n",
            "loss in epoch 5 , step 380 : 0.603802\n",
            "loss in epoch 5 , step 400 : 0.607711\n",
            "loss in epoch 5 , step 420 : 0.380529\n",
            "loss in epoch 5 , step 440 : 0.290060\n",
            "loss in epoch 5 , step 460 : 0.505472\n",
            "loss in epoch 5 , step 480 : 0.485769\n",
            "loss in epoch 5 , step 500 : 0.455772\n",
            "loss in epoch 5 , step 520 : 0.487587\n",
            "loss in epoch 5 , step 540 : 0.409384\n",
            "loss in epoch 5 , step 560 : 0.507461\n",
            "loss in epoch 5 , step 580 : 0.428638\n",
            "loss in epoch 5 , step 600 : 0.427321\n",
            "loss in epoch 5 , step 620 : 0.584567\n",
            "loss in epoch 5 , step 640 : 0.623369\n",
            "loss in epoch 5 , step 660 : 0.304046\n",
            "loss in epoch 5 , step 680 : 0.297929\n",
            "loss in epoch 5 , step 700 : 0.501004\n",
            "loss in epoch 5 , step 720 : 0.447670\n",
            "loss in epoch 5 , step 740 : 0.449122\n",
            "loss in epoch 5 , step 760 : 0.521100\n",
            "loss in epoch 5 , step 780 : 0.386349\n",
            "Accuracy in epoch 5 : 77.370003\n",
            "loss in epoch 6 , step 0 : 0.305693\n",
            "loss in epoch 6 , step 20 : 0.379381\n",
            "loss in epoch 6 , step 40 : 0.263347\n",
            "loss in epoch 6 , step 60 : 0.303180\n",
            "loss in epoch 6 , step 80 : 0.529272\n",
            "loss in epoch 6 , step 100 : 0.429237\n",
            "loss in epoch 6 , step 120 : 0.476367\n",
            "loss in epoch 6 , step 140 : 0.500298\n",
            "loss in epoch 6 , step 160 : 0.452359\n",
            "loss in epoch 6 , step 180 : 0.451371\n",
            "loss in epoch 6 , step 200 : 0.529181\n",
            "loss in epoch 6 , step 220 : 0.315395\n",
            "loss in epoch 6 , step 240 : 0.429391\n",
            "loss in epoch 6 , step 260 : 0.384979\n",
            "loss in epoch 6 , step 280 : 0.388934\n",
            "loss in epoch 6 , step 300 : 0.531587\n",
            "loss in epoch 6 , step 320 : 0.447975\n",
            "loss in epoch 6 , step 340 : 0.490297\n",
            "loss in epoch 6 , step 360 : 0.529308\n",
            "loss in epoch 6 , step 380 : 0.448653\n",
            "loss in epoch 6 , step 400 : 0.512461\n",
            "loss in epoch 6 , step 420 : 0.533711\n",
            "loss in epoch 6 , step 440 : 0.509449\n",
            "loss in epoch 6 , step 460 : 0.326347\n",
            "loss in epoch 6 , step 480 : 0.627459\n",
            "loss in epoch 6 , step 500 : 0.608212\n",
            "loss in epoch 6 , step 520 : 0.414697\n",
            "loss in epoch 6 , step 540 : 0.384692\n",
            "loss in epoch 6 , step 560 : 0.507036\n",
            "loss in epoch 6 , step 580 : 0.326973\n",
            "loss in epoch 6 , step 600 : 0.441165\n",
            "loss in epoch 6 , step 620 : 0.247703\n",
            "loss in epoch 6 , step 640 : 0.324068\n",
            "loss in epoch 6 , step 660 : 0.434751\n",
            "loss in epoch 6 , step 680 : 0.343643\n",
            "loss in epoch 6 , step 700 : 0.308780\n",
            "loss in epoch 6 , step 720 : 0.546097\n",
            "loss in epoch 6 , step 740 : 0.516198\n",
            "loss in epoch 6 , step 760 : 0.506803\n",
            "loss in epoch 6 , step 780 : 0.547039\n",
            "Accuracy in epoch 6 : 72.959999\n",
            "loss in epoch 7 , step 0 : 0.380007\n",
            "loss in epoch 7 , step 20 : 0.417409\n",
            "loss in epoch 7 , step 40 : 0.533314\n",
            "loss in epoch 7 , step 60 : 0.471990\n",
            "loss in epoch 7 , step 80 : 0.352175\n",
            "loss in epoch 7 , step 100 : 0.614606\n",
            "loss in epoch 7 , step 120 : 0.580296\n",
            "loss in epoch 7 , step 140 : 0.385832\n",
            "loss in epoch 7 , step 160 : 0.440738\n",
            "loss in epoch 7 , step 180 : 0.330584\n",
            "loss in epoch 7 , step 200 : 0.219962\n",
            "loss in epoch 7 , step 220 : 0.404644\n",
            "loss in epoch 7 , step 240 : 0.400930\n",
            "loss in epoch 7 , step 260 : 0.495787\n",
            "loss in epoch 7 , step 280 : 0.425546\n",
            "loss in epoch 7 , step 300 : 0.416295\n",
            "loss in epoch 7 , step 320 : 0.500459\n",
            "loss in epoch 7 , step 340 : 0.517277\n",
            "loss in epoch 7 , step 360 : 0.422796\n",
            "loss in epoch 7 , step 380 : 0.520739\n",
            "loss in epoch 7 , step 400 : 0.411886\n",
            "loss in epoch 7 , step 420 : 0.557049\n",
            "loss in epoch 7 , step 440 : 0.427870\n",
            "loss in epoch 7 , step 460 : 0.355217\n",
            "loss in epoch 7 , step 480 : 0.617226\n",
            "loss in epoch 7 , step 500 : 0.528287\n",
            "loss in epoch 7 , step 520 : 0.327678\n",
            "loss in epoch 7 , step 540 : 0.395463\n",
            "loss in epoch 7 , step 560 : 0.307816\n",
            "loss in epoch 7 , step 580 : 0.354076\n",
            "loss in epoch 7 , step 600 : 0.511277\n",
            "loss in epoch 7 , step 620 : 0.373912\n",
            "loss in epoch 7 , step 640 : 0.345624\n",
            "loss in epoch 7 , step 660 : 0.584457\n",
            "loss in epoch 7 , step 680 : 0.564056\n",
            "loss in epoch 7 , step 700 : 0.540742\n",
            "loss in epoch 7 , step 720 : 0.363503\n",
            "loss in epoch 7 , step 740 : 0.392849\n",
            "loss in epoch 7 , step 760 : 0.325821\n",
            "loss in epoch 7 , step 780 : 0.464544\n",
            "Accuracy in epoch 7 : 82.879997\n",
            "loss in epoch 8 , step 0 : 0.363480\n",
            "loss in epoch 8 , step 20 : 0.538264\n",
            "loss in epoch 8 , step 40 : 0.449300\n",
            "loss in epoch 8 , step 60 : 0.531543\n",
            "loss in epoch 8 , step 80 : 0.342250\n",
            "loss in epoch 8 , step 100 : 0.433166\n",
            "loss in epoch 8 , step 120 : 0.323069\n",
            "loss in epoch 8 , step 140 : 0.517759\n",
            "loss in epoch 8 , step 160 : 0.311895\n",
            "loss in epoch 8 , step 180 : 0.426669\n",
            "loss in epoch 8 , step 200 : 0.379197\n",
            "loss in epoch 8 , step 220 : 0.395353\n",
            "loss in epoch 8 , step 240 : 0.409789\n",
            "loss in epoch 8 , step 260 : 0.499319\n",
            "loss in epoch 8 , step 280 : 0.518204\n",
            "loss in epoch 8 , step 300 : 0.384250\n",
            "loss in epoch 8 , step 320 : 0.504508\n",
            "loss in epoch 8 , step 340 : 0.210852\n",
            "loss in epoch 8 , step 360 : 0.527514\n",
            "loss in epoch 8 , step 380 : 0.451660\n",
            "loss in epoch 8 , step 400 : 0.274287\n",
            "loss in epoch 8 , step 420 : 0.356520\n",
            "loss in epoch 8 , step 440 : 0.214975\n",
            "loss in epoch 8 , step 460 : 0.450227\n",
            "loss in epoch 8 , step 480 : 0.303850\n",
            "loss in epoch 8 , step 500 : 0.426650\n",
            "loss in epoch 8 , step 520 : 0.499659\n",
            "loss in epoch 8 , step 540 : 0.323371\n",
            "loss in epoch 8 , step 560 : 0.558039\n",
            "loss in epoch 8 , step 580 : 0.525337\n",
            "loss in epoch 8 , step 600 : 0.403336\n",
            "loss in epoch 8 , step 620 : 0.442348\n",
            "loss in epoch 8 , step 640 : 0.278437\n",
            "loss in epoch 8 , step 660 : 0.305318\n",
            "loss in epoch 8 , step 680 : 0.505519\n",
            "loss in epoch 8 , step 700 : 0.523014\n",
            "loss in epoch 8 , step 720 : 0.344560\n",
            "loss in epoch 8 , step 740 : 0.315762\n",
            "loss in epoch 8 , step 760 : 0.371337\n",
            "loss in epoch 8 , step 780 : 0.309584\n",
            "Accuracy in epoch 8 : 80.589996\n",
            "loss in epoch 9 , step 0 : 0.308455\n",
            "loss in epoch 9 , step 20 : 0.205041\n",
            "loss in epoch 9 , step 40 : 0.401399\n",
            "loss in epoch 9 , step 60 : 0.230854\n",
            "loss in epoch 9 , step 80 : 0.369400\n",
            "loss in epoch 9 , step 100 : 0.331622\n",
            "loss in epoch 9 , step 120 : 0.515518\n",
            "loss in epoch 9 , step 140 : 0.520901\n",
            "loss in epoch 9 , step 160 : 0.352009\n",
            "loss in epoch 9 , step 180 : 0.324743\n",
            "loss in epoch 9 , step 200 : 0.427458\n",
            "loss in epoch 9 , step 220 : 0.558224\n",
            "loss in epoch 9 , step 240 : 0.521033\n",
            "loss in epoch 9 , step 260 : 0.372528\n",
            "loss in epoch 9 , step 280 : 0.211094\n",
            "loss in epoch 9 , step 300 : 0.419064\n",
            "loss in epoch 9 , step 320 : 0.361999\n",
            "loss in epoch 9 , step 340 : 0.403249\n",
            "loss in epoch 9 , step 360 : 0.287273\n",
            "loss in epoch 9 , step 380 : 0.425758\n",
            "loss in epoch 9 , step 400 : 0.283916\n",
            "loss in epoch 9 , step 420 : 0.286802\n",
            "loss in epoch 9 , step 440 : 0.300252\n",
            "loss in epoch 9 , step 460 : 0.348797\n",
            "loss in epoch 9 , step 480 : 0.459152\n",
            "loss in epoch 9 , step 500 : 0.424197\n",
            "loss in epoch 9 , step 520 : 0.440375\n",
            "loss in epoch 9 , step 540 : 0.330041\n",
            "loss in epoch 9 , step 560 : 0.562424\n",
            "loss in epoch 9 , step 580 : 0.395255\n",
            "loss in epoch 9 , step 600 : 0.356939\n",
            "loss in epoch 9 , step 620 : 0.390864\n",
            "loss in epoch 9 , step 640 : 0.287746\n",
            "loss in epoch 9 , step 660 : 0.206049\n",
            "loss in epoch 9 , step 680 : 0.446680\n",
            "loss in epoch 9 , step 700 : 0.330617\n",
            "loss in epoch 9 , step 720 : 0.393676\n",
            "loss in epoch 9 , step 740 : 0.395236\n",
            "loss in epoch 9 , step 760 : 0.368131\n",
            "loss in epoch 9 , step 780 : 0.382168\n",
            "Accuracy in epoch 9 : 83.599998\n",
            "loss in epoch 10 , step 0 : 0.418667\n",
            "loss in epoch 10 , step 20 : 0.327655\n",
            "loss in epoch 10 , step 40 : 0.320192\n",
            "loss in epoch 10 , step 60 : 0.323316\n",
            "loss in epoch 10 , step 80 : 0.301614\n",
            "loss in epoch 10 , step 100 : 0.325766\n",
            "loss in epoch 10 , step 120 : 0.237848\n",
            "loss in epoch 10 , step 140 : 0.394366\n",
            "loss in epoch 10 , step 160 : 0.236919\n",
            "loss in epoch 10 , step 180 : 0.537834\n",
            "loss in epoch 10 , step 200 : 0.301437\n",
            "loss in epoch 10 , step 220 : 0.407213\n",
            "loss in epoch 10 , step 240 : 0.374943\n",
            "loss in epoch 10 , step 260 : 0.511854\n",
            "loss in epoch 10 , step 280 : 0.269851\n",
            "loss in epoch 10 , step 300 : 0.314897\n",
            "loss in epoch 10 , step 320 : 0.414940\n",
            "loss in epoch 10 , step 340 : 0.351715\n",
            "loss in epoch 10 , step 360 : 0.402201\n",
            "loss in epoch 10 , step 380 : 0.593309\n",
            "loss in epoch 10 , step 400 : 0.382536\n",
            "loss in epoch 10 , step 420 : 0.356390\n",
            "loss in epoch 10 , step 440 : 0.443637\n",
            "loss in epoch 10 , step 460 : 0.340670\n",
            "loss in epoch 10 , step 480 : 0.469263\n",
            "loss in epoch 10 , step 500 : 0.296401\n",
            "loss in epoch 10 , step 520 : 0.383225\n",
            "loss in epoch 10 , step 540 : 0.394529\n",
            "loss in epoch 10 , step 560 : 0.370126\n",
            "loss in epoch 10 , step 580 : 0.336486\n",
            "loss in epoch 10 , step 600 : 0.357827\n",
            "loss in epoch 10 , step 620 : 0.323868\n",
            "loss in epoch 10 , step 640 : 0.566219\n",
            "loss in epoch 10 , step 660 : 0.392587\n",
            "loss in epoch 10 , step 680 : 0.661013\n",
            "loss in epoch 10 , step 700 : 0.209526\n",
            "loss in epoch 10 , step 720 : 0.295904\n",
            "loss in epoch 10 , step 740 : 0.293133\n",
            "loss in epoch 10 , step 760 : 0.292291\n",
            "loss in epoch 10 , step 780 : 0.352273\n",
            "Accuracy in epoch 10 : 81.089996\n",
            "loss in epoch 11 , step 0 : 0.313524\n",
            "loss in epoch 11 , step 20 : 0.352438\n",
            "loss in epoch 11 , step 40 : 0.380814\n",
            "loss in epoch 11 , step 60 : 0.264620\n",
            "loss in epoch 11 , step 80 : 0.411036\n",
            "loss in epoch 11 , step 100 : 0.354559\n",
            "loss in epoch 11 , step 120 : 0.406558\n",
            "loss in epoch 11 , step 140 : 0.430773\n",
            "loss in epoch 11 , step 160 : 0.407405\n",
            "loss in epoch 11 , step 180 : 0.656797\n",
            "loss in epoch 11 , step 200 : 0.454073\n",
            "loss in epoch 11 , step 220 : 0.468204\n",
            "loss in epoch 11 , step 240 : 0.215061\n",
            "loss in epoch 11 , step 260 : 0.359540\n",
            "loss in epoch 11 , step 280 : 0.435222\n",
            "loss in epoch 11 , step 300 : 0.321369\n",
            "loss in epoch 11 , step 320 : 0.193726\n",
            "loss in epoch 11 , step 340 : 0.270862\n",
            "loss in epoch 11 , step 360 : 0.193057\n",
            "loss in epoch 11 , step 380 : 0.520442\n",
            "loss in epoch 11 , step 400 : 0.317086\n",
            "loss in epoch 11 , step 420 : 0.185863\n",
            "loss in epoch 11 , step 440 : 0.489075\n",
            "loss in epoch 11 , step 460 : 0.332284\n",
            "loss in epoch 11 , step 480 : 0.364807\n",
            "loss in epoch 11 , step 500 : 0.470928\n",
            "loss in epoch 11 , step 520 : 0.246557\n",
            "loss in epoch 11 , step 540 : 0.443833\n",
            "loss in epoch 11 , step 560 : 0.376130\n",
            "loss in epoch 11 , step 580 : 0.441845\n",
            "loss in epoch 11 , step 600 : 0.557480\n",
            "loss in epoch 11 , step 620 : 0.508971\n",
            "loss in epoch 11 , step 640 : 0.182586\n",
            "loss in epoch 11 , step 660 : 0.332896\n",
            "loss in epoch 11 , step 680 : 0.450670\n",
            "loss in epoch 11 , step 700 : 0.198192\n",
            "loss in epoch 11 , step 720 : 0.221048\n",
            "loss in epoch 11 , step 740 : 0.346539\n",
            "loss in epoch 11 , step 760 : 0.394764\n",
            "loss in epoch 11 , step 780 : 0.474255\n",
            "Accuracy in epoch 11 : 81.120003\n",
            "loss in epoch 12 , step 0 : 0.340612\n",
            "loss in epoch 12 , step 20 : 0.359335\n",
            "loss in epoch 12 , step 40 : 0.318462\n",
            "loss in epoch 12 , step 60 : 0.390626\n",
            "loss in epoch 12 , step 80 : 0.283763\n",
            "loss in epoch 12 , step 100 : 0.415131\n",
            "loss in epoch 12 , step 120 : 0.377003\n",
            "loss in epoch 12 , step 140 : 0.341385\n",
            "loss in epoch 12 , step 160 : 0.235679\n",
            "loss in epoch 12 , step 180 : 0.232813\n",
            "loss in epoch 12 , step 200 : 0.511639\n",
            "loss in epoch 12 , step 220 : 0.190711\n",
            "loss in epoch 12 , step 240 : 0.288022\n",
            "loss in epoch 12 , step 260 : 0.295158\n",
            "loss in epoch 12 , step 280 : 0.278145\n",
            "loss in epoch 12 , step 300 : 0.286062\n",
            "loss in epoch 12 , step 320 : 0.195912\n",
            "loss in epoch 12 , step 340 : 0.276690\n",
            "loss in epoch 12 , step 360 : 0.332929\n",
            "loss in epoch 12 , step 380 : 0.255785\n",
            "loss in epoch 12 , step 400 : 0.377636\n",
            "loss in epoch 12 , step 420 : 0.233133\n",
            "loss in epoch 12 , step 440 : 0.411031\n",
            "loss in epoch 12 , step 460 : 0.476364\n",
            "loss in epoch 12 , step 480 : 0.379038\n",
            "loss in epoch 12 , step 500 : 0.342248\n",
            "loss in epoch 12 , step 520 : 0.172397\n",
            "loss in epoch 12 , step 540 : 0.318606\n",
            "loss in epoch 12 , step 560 : 0.434327\n",
            "loss in epoch 12 , step 580 : 0.273728\n",
            "loss in epoch 12 , step 600 : 0.374565\n",
            "loss in epoch 12 , step 620 : 0.519536\n",
            "loss in epoch 12 , step 640 : 0.265134\n",
            "loss in epoch 12 , step 660 : 0.572034\n",
            "loss in epoch 12 , step 680 : 0.199272\n",
            "loss in epoch 12 , step 700 : 0.542934\n",
            "loss in epoch 12 , step 720 : 0.502420\n",
            "loss in epoch 12 , step 740 : 0.189421\n",
            "loss in epoch 12 , step 760 : 0.429116\n",
            "loss in epoch 12 , step 780 : 0.430784\n",
            "Accuracy in epoch 12 : 84.550003\n",
            "loss in epoch 13 , step 0 : 0.251698\n",
            "loss in epoch 13 , step 20 : 0.308737\n",
            "loss in epoch 13 , step 40 : 0.370620\n",
            "loss in epoch 13 , step 60 : 0.441996\n",
            "loss in epoch 13 , step 80 : 0.234085\n",
            "loss in epoch 13 , step 100 : 0.262278\n",
            "loss in epoch 13 , step 120 : 0.249379\n",
            "loss in epoch 13 , step 140 : 0.633114\n",
            "loss in epoch 13 , step 160 : 0.420047\n",
            "loss in epoch 13 , step 180 : 0.507998\n",
            "loss in epoch 13 , step 200 : 0.533911\n",
            "loss in epoch 13 , step 220 : 0.210711\n",
            "loss in epoch 13 , step 240 : 0.258590\n",
            "loss in epoch 13 , step 260 : 0.537234\n",
            "loss in epoch 13 , step 280 : 0.350287\n",
            "loss in epoch 13 , step 300 : 0.287207\n",
            "loss in epoch 13 , step 320 : 0.301819\n",
            "loss in epoch 13 , step 340 : 0.542810\n",
            "loss in epoch 13 , step 360 : 0.253277\n",
            "loss in epoch 13 , step 380 : 0.329143\n",
            "loss in epoch 13 , step 400 : 0.294449\n",
            "loss in epoch 13 , step 420 : 0.399900\n",
            "loss in epoch 13 , step 440 : 0.281478\n",
            "loss in epoch 13 , step 460 : 0.228701\n",
            "loss in epoch 13 , step 480 : 0.348757\n",
            "loss in epoch 13 , step 500 : 0.410462\n",
            "loss in epoch 13 , step 520 : 0.313157\n",
            "loss in epoch 13 , step 540 : 0.276205\n",
            "loss in epoch 13 , step 560 : 0.382409\n",
            "loss in epoch 13 , step 580 : 0.334148\n",
            "loss in epoch 13 , step 600 : 0.323956\n",
            "loss in epoch 13 , step 620 : 0.200598\n",
            "loss in epoch 13 , step 640 : 0.398990\n",
            "loss in epoch 13 , step 660 : 0.400873\n",
            "loss in epoch 13 , step 680 : 0.618206\n",
            "loss in epoch 13 , step 700 : 0.232659\n",
            "loss in epoch 13 , step 720 : 0.321905\n",
            "loss in epoch 13 , step 740 : 0.344351\n",
            "loss in epoch 13 , step 760 : 0.368879\n",
            "loss in epoch 13 , step 780 : 0.216282\n",
            "Accuracy in epoch 13 : 79.430000\n",
            "loss in epoch 14 , step 0 : 0.219324\n",
            "loss in epoch 14 , step 20 : 0.198647\n",
            "loss in epoch 14 , step 40 : 0.338925\n",
            "loss in epoch 14 , step 60 : 0.261848\n",
            "loss in epoch 14 , step 80 : 0.239723\n",
            "loss in epoch 14 , step 100 : 0.400959\n",
            "loss in epoch 14 , step 120 : 0.302254\n",
            "loss in epoch 14 , step 140 : 0.456104\n",
            "loss in epoch 14 , step 160 : 0.409156\n",
            "loss in epoch 14 , step 180 : 0.340551\n",
            "loss in epoch 14 , step 200 : 0.443127\n",
            "loss in epoch 14 , step 220 : 0.206426\n",
            "loss in epoch 14 , step 240 : 0.544175\n",
            "loss in epoch 14 , step 260 : 0.297360\n",
            "loss in epoch 14 , step 280 : 0.174115\n",
            "loss in epoch 14 , step 300 : 0.092378\n",
            "loss in epoch 14 , step 320 : 0.362026\n",
            "loss in epoch 14 , step 340 : 0.332543\n",
            "loss in epoch 14 , step 360 : 0.196149\n",
            "loss in epoch 14 , step 380 : 0.546497\n",
            "loss in epoch 14 , step 400 : 0.439425\n",
            "loss in epoch 14 , step 420 : 0.383821\n",
            "loss in epoch 14 , step 440 : 0.248639\n",
            "loss in epoch 14 , step 460 : 0.306217\n",
            "loss in epoch 14 , step 480 : 0.233195\n",
            "loss in epoch 14 , step 500 : 0.324521\n",
            "loss in epoch 14 , step 520 : 0.192822\n",
            "loss in epoch 14 , step 540 : 0.160555\n",
            "loss in epoch 14 , step 560 : 0.159063\n",
            "loss in epoch 14 , step 580 : 0.219241\n",
            "loss in epoch 14 , step 600 : 0.447956\n",
            "loss in epoch 14 , step 620 : 0.404486\n",
            "loss in epoch 14 , step 640 : 0.420415\n",
            "loss in epoch 14 , step 660 : 0.513666\n",
            "loss in epoch 14 , step 680 : 0.408455\n",
            "loss in epoch 14 , step 700 : 0.300907\n",
            "loss in epoch 14 , step 720 : 0.308964\n",
            "loss in epoch 14 , step 740 : 0.176010\n",
            "loss in epoch 14 , step 760 : 0.248307\n",
            "loss in epoch 14 , step 780 : 0.472085\n",
            "Accuracy in epoch 14 : 84.779999\n",
            "loss in epoch 15 , step 0 : 0.313021\n",
            "loss in epoch 15 , step 20 : 0.274509\n",
            "loss in epoch 15 , step 40 : 0.252295\n",
            "loss in epoch 15 , step 60 : 0.444661\n",
            "loss in epoch 15 , step 80 : 0.340915\n",
            "loss in epoch 15 , step 100 : 0.278965\n",
            "loss in epoch 15 , step 120 : 0.293556\n",
            "loss in epoch 15 , step 140 : 0.526656\n",
            "loss in epoch 15 , step 160 : 0.315591\n",
            "loss in epoch 15 , step 180 : 0.343448\n",
            "loss in epoch 15 , step 200 : 0.293605\n",
            "loss in epoch 15 , step 220 : 0.188706\n",
            "loss in epoch 15 , step 240 : 0.245358\n",
            "loss in epoch 15 , step 260 : 0.304905\n",
            "loss in epoch 15 , step 280 : 0.343174\n",
            "loss in epoch 15 , step 300 : 0.481234\n",
            "loss in epoch 15 , step 320 : 0.374491\n",
            "loss in epoch 15 , step 340 : 0.252641\n",
            "loss in epoch 15 , step 360 : 0.292630\n",
            "loss in epoch 15 , step 380 : 0.556590\n",
            "loss in epoch 15 , step 400 : 0.313728\n",
            "loss in epoch 15 , step 420 : 0.302925\n",
            "loss in epoch 15 , step 440 : 0.585397\n",
            "loss in epoch 15 , step 460 : 0.128289\n",
            "loss in epoch 15 , step 480 : 0.318712\n",
            "loss in epoch 15 , step 500 : 0.275545\n",
            "loss in epoch 15 , step 520 : 0.337900\n",
            "loss in epoch 15 , step 540 : 0.249134\n",
            "loss in epoch 15 , step 560 : 0.320766\n",
            "loss in epoch 15 , step 580 : 0.345487\n",
            "loss in epoch 15 , step 600 : 0.286401\n",
            "loss in epoch 15 , step 620 : 0.241756\n",
            "loss in epoch 15 , step 640 : 0.340557\n",
            "loss in epoch 15 , step 660 : 0.142104\n",
            "loss in epoch 15 , step 680 : 0.356495\n",
            "loss in epoch 15 , step 700 : 0.500834\n",
            "loss in epoch 15 , step 720 : 0.274987\n",
            "loss in epoch 15 , step 740 : 0.249049\n",
            "loss in epoch 15 , step 760 : 0.510465\n",
            "loss in epoch 15 , step 780 : 0.319581\n",
            "Accuracy in epoch 15 : 86.169998\n",
            "loss in epoch 16 , step 0 : 0.326023\n",
            "loss in epoch 16 , step 20 : 0.329135\n",
            "loss in epoch 16 , step 40 : 0.345949\n",
            "loss in epoch 16 , step 60 : 0.182521\n",
            "loss in epoch 16 , step 80 : 0.184633\n",
            "loss in epoch 16 , step 100 : 0.270973\n",
            "loss in epoch 16 , step 120 : 0.221786\n",
            "loss in epoch 16 , step 140 : 0.306638\n",
            "loss in epoch 16 , step 160 : 0.302465\n",
            "loss in epoch 16 , step 180 : 0.289129\n",
            "loss in epoch 16 , step 200 : 0.209718\n",
            "loss in epoch 16 , step 220 : 0.349014\n",
            "loss in epoch 16 , step 240 : 0.287843\n",
            "loss in epoch 16 , step 260 : 0.338542\n",
            "loss in epoch 16 , step 280 : 0.205953\n",
            "loss in epoch 16 , step 300 : 0.353063\n",
            "loss in epoch 16 , step 320 : 0.367164\n",
            "loss in epoch 16 , step 340 : 0.391814\n",
            "loss in epoch 16 , step 360 : 0.383893\n",
            "loss in epoch 16 , step 380 : 0.375374\n",
            "loss in epoch 16 , step 400 : 0.300239\n",
            "loss in epoch 16 , step 420 : 0.409695\n",
            "loss in epoch 16 , step 440 : 0.500729\n",
            "loss in epoch 16 , step 460 : 0.230235\n",
            "loss in epoch 16 , step 480 : 0.194416\n",
            "loss in epoch 16 , step 500 : 0.267955\n",
            "loss in epoch 16 , step 520 : 0.246723\n",
            "loss in epoch 16 , step 540 : 0.426007\n",
            "loss in epoch 16 , step 560 : 0.402520\n",
            "loss in epoch 16 , step 580 : 0.251607\n",
            "loss in epoch 16 , step 600 : 0.319677\n",
            "loss in epoch 16 , step 620 : 0.497644\n",
            "loss in epoch 16 , step 640 : 0.467334\n",
            "loss in epoch 16 , step 660 : 0.270816\n",
            "loss in epoch 16 , step 680 : 0.389124\n",
            "loss in epoch 16 , step 700 : 0.273729\n",
            "loss in epoch 16 , step 720 : 0.279839\n",
            "loss in epoch 16 , step 740 : 0.346749\n",
            "loss in epoch 16 , step 760 : 0.406245\n",
            "loss in epoch 16 , step 780 : 0.338733\n",
            "Accuracy in epoch 16 : 85.750000\n",
            "loss in epoch 17 , step 0 : 0.317265\n",
            "loss in epoch 17 , step 20 : 0.290832\n",
            "loss in epoch 17 , step 40 : 0.342817\n",
            "loss in epoch 17 , step 60 : 0.145265\n",
            "loss in epoch 17 , step 80 : 0.220871\n",
            "loss in epoch 17 , step 100 : 0.280678\n",
            "loss in epoch 17 , step 120 : 0.259507\n",
            "loss in epoch 17 , step 140 : 0.265341\n",
            "loss in epoch 17 , step 160 : 0.228723\n",
            "loss in epoch 17 , step 180 : 0.370611\n",
            "loss in epoch 17 , step 200 : 0.387108\n",
            "loss in epoch 17 , step 220 : 0.308794\n",
            "loss in epoch 17 , step 240 : 0.390738\n",
            "loss in epoch 17 , step 260 : 0.220698\n",
            "loss in epoch 17 , step 280 : 0.227945\n",
            "loss in epoch 17 , step 300 : 0.296607\n",
            "loss in epoch 17 , step 320 : 0.208144\n",
            "loss in epoch 17 , step 340 : 0.167568\n",
            "loss in epoch 17 , step 360 : 0.241258\n",
            "loss in epoch 17 , step 380 : 0.162231\n",
            "loss in epoch 17 , step 400 : 0.382874\n",
            "loss in epoch 17 , step 420 : 0.390202\n",
            "loss in epoch 17 , step 440 : 0.310872\n",
            "loss in epoch 17 , step 460 : 0.391050\n",
            "loss in epoch 17 , step 480 : 0.623931\n",
            "loss in epoch 17 , step 500 : 0.220690\n",
            "loss in epoch 17 , step 520 : 0.149513\n",
            "loss in epoch 17 , step 540 : 0.230408\n",
            "loss in epoch 17 , step 560 : 0.420923\n",
            "loss in epoch 17 , step 580 : 0.297545\n",
            "loss in epoch 17 , step 600 : 0.259762\n",
            "loss in epoch 17 , step 620 : 0.363740\n",
            "loss in epoch 17 , step 640 : 0.121821\n",
            "loss in epoch 17 , step 660 : 0.298508\n",
            "loss in epoch 17 , step 680 : 0.496172\n",
            "loss in epoch 17 , step 700 : 0.494321\n",
            "loss in epoch 17 , step 720 : 0.304489\n",
            "loss in epoch 17 , step 740 : 0.264508\n",
            "loss in epoch 17 , step 760 : 0.311797\n",
            "loss in epoch 17 , step 780 : 0.366042\n",
            "Accuracy in epoch 17 : 84.360001\n",
            "loss in epoch 18 , step 0 : 0.308498\n",
            "loss in epoch 18 , step 20 : 0.210413\n",
            "loss in epoch 18 , step 40 : 0.318015\n",
            "loss in epoch 18 , step 60 : 0.225309\n",
            "loss in epoch 18 , step 80 : 0.272343\n",
            "loss in epoch 18 , step 100 : 0.236039\n",
            "loss in epoch 18 , step 120 : 0.202864\n",
            "loss in epoch 18 , step 140 : 0.298930\n",
            "loss in epoch 18 , step 160 : 0.366786\n",
            "loss in epoch 18 , step 180 : 0.296819\n",
            "loss in epoch 18 , step 200 : 0.333980\n",
            "loss in epoch 18 , step 220 : 0.214703\n",
            "loss in epoch 18 , step 240 : 0.350912\n",
            "loss in epoch 18 , step 260 : 0.370391\n",
            "loss in epoch 18 , step 280 : 0.415949\n",
            "loss in epoch 18 , step 300 : 0.245067\n",
            "loss in epoch 18 , step 320 : 0.275909\n",
            "loss in epoch 18 , step 340 : 0.227731\n",
            "loss in epoch 18 , step 360 : 0.264366\n",
            "loss in epoch 18 , step 380 : 0.284277\n",
            "loss in epoch 18 , step 400 : 0.268243\n",
            "loss in epoch 18 , step 420 : 0.319171\n",
            "loss in epoch 18 , step 440 : 0.229972\n",
            "loss in epoch 18 , step 460 : 0.246223\n",
            "loss in epoch 18 , step 480 : 0.219037\n",
            "loss in epoch 18 , step 500 : 0.347218\n",
            "loss in epoch 18 , step 520 : 0.318138\n",
            "loss in epoch 18 , step 540 : 0.362191\n",
            "loss in epoch 18 , step 560 : 0.425760\n",
            "loss in epoch 18 , step 580 : 0.324154\n",
            "loss in epoch 18 , step 600 : 0.398439\n",
            "loss in epoch 18 , step 620 : 0.291691\n",
            "loss in epoch 18 , step 640 : 0.393626\n",
            "loss in epoch 18 , step 660 : 0.400326\n",
            "loss in epoch 18 , step 680 : 0.367437\n",
            "loss in epoch 18 , step 700 : 0.309775\n",
            "loss in epoch 18 , step 720 : 0.411299\n",
            "loss in epoch 18 , step 740 : 0.408229\n",
            "loss in epoch 18 , step 760 : 0.172470\n",
            "loss in epoch 18 , step 780 : 0.283665\n",
            "Accuracy in epoch 18 : 86.860001\n",
            "loss in epoch 19 , step 0 : 0.200720\n",
            "loss in epoch 19 , step 20 : 0.217541\n",
            "loss in epoch 19 , step 40 : 0.234510\n",
            "loss in epoch 19 , step 60 : 0.316079\n",
            "loss in epoch 19 , step 80 : 0.164103\n",
            "loss in epoch 19 , step 100 : 0.321444\n",
            "loss in epoch 19 , step 120 : 0.175389\n",
            "loss in epoch 19 , step 140 : 0.383359\n",
            "loss in epoch 19 , step 160 : 0.251022\n",
            "loss in epoch 19 , step 180 : 0.270988\n",
            "loss in epoch 19 , step 200 : 0.433269\n",
            "loss in epoch 19 , step 220 : 0.281167\n",
            "loss in epoch 19 , step 240 : 0.474940\n",
            "loss in epoch 19 , step 260 : 0.368494\n",
            "loss in epoch 19 , step 280 : 0.400186\n",
            "loss in epoch 19 , step 300 : 0.327286\n",
            "loss in epoch 19 , step 320 : 0.405925\n",
            "loss in epoch 19 , step 340 : 0.206494\n",
            "loss in epoch 19 , step 360 : 0.284737\n",
            "loss in epoch 19 , step 380 : 0.134384\n",
            "loss in epoch 19 , step 400 : 0.272994\n",
            "loss in epoch 19 , step 420 : 0.249058\n",
            "loss in epoch 19 , step 440 : 0.174351\n",
            "loss in epoch 19 , step 460 : 0.317798\n",
            "loss in epoch 19 , step 480 : 0.180562\n",
            "loss in epoch 19 , step 500 : 0.337805\n",
            "loss in epoch 19 , step 520 : 0.295321\n",
            "loss in epoch 19 , step 540 : 0.381218\n",
            "loss in epoch 19 , step 560 : 0.375664\n",
            "loss in epoch 19 , step 580 : 0.401064\n",
            "loss in epoch 19 , step 600 : 0.220135\n",
            "loss in epoch 19 , step 620 : 0.348395\n",
            "loss in epoch 19 , step 640 : 0.187030\n",
            "loss in epoch 19 , step 660 : 0.348657\n",
            "loss in epoch 19 , step 680 : 0.250013\n",
            "loss in epoch 19 , step 700 : 0.318709\n",
            "loss in epoch 19 , step 720 : 0.331941\n",
            "loss in epoch 19 , step 740 : 0.187739\n",
            "loss in epoch 19 , step 760 : 0.206961\n",
            "loss in epoch 19 , step 780 : 0.264832\n",
            "Accuracy in epoch 19 : 84.919998\n",
            "loss in epoch 20 , step 0 : 0.390653\n",
            "loss in epoch 20 , step 20 : 0.415421\n",
            "loss in epoch 20 , step 40 : 0.181303\n",
            "loss in epoch 20 , step 60 : 0.268953\n",
            "loss in epoch 20 , step 80 : 0.342096\n",
            "loss in epoch 20 , step 100 : 0.220607\n",
            "loss in epoch 20 , step 120 : 0.248837\n",
            "loss in epoch 20 , step 140 : 0.316091\n",
            "loss in epoch 20 , step 160 : 0.468433\n",
            "loss in epoch 20 , step 180 : 0.424964\n",
            "loss in epoch 20 , step 200 : 0.302996\n",
            "loss in epoch 20 , step 220 : 0.369219\n",
            "loss in epoch 20 , step 240 : 0.195197\n",
            "loss in epoch 20 , step 260 : 0.214556\n",
            "loss in epoch 20 , step 280 : 0.347826\n",
            "loss in epoch 20 , step 300 : 0.222440\n",
            "loss in epoch 20 , step 320 : 0.186927\n",
            "loss in epoch 20 , step 340 : 0.345409\n",
            "loss in epoch 20 , step 360 : 0.375466\n",
            "loss in epoch 20 , step 380 : 0.188462\n",
            "loss in epoch 20 , step 400 : 0.357806\n",
            "loss in epoch 20 , step 420 : 0.418004\n",
            "loss in epoch 20 , step 440 : 0.394214\n",
            "loss in epoch 20 , step 460 : 0.324924\n",
            "loss in epoch 20 , step 480 : 0.340848\n",
            "loss in epoch 20 , step 500 : 0.287013\n",
            "loss in epoch 20 , step 520 : 0.339689\n",
            "loss in epoch 20 , step 540 : 0.288681\n",
            "loss in epoch 20 , step 560 : 0.306217\n",
            "loss in epoch 20 , step 580 : 0.372340\n",
            "loss in epoch 20 , step 600 : 0.280802\n",
            "loss in epoch 20 , step 620 : 0.426827\n",
            "loss in epoch 20 , step 640 : 0.324321\n",
            "loss in epoch 20 , step 660 : 0.358058\n",
            "loss in epoch 20 , step 680 : 0.451636\n",
            "loss in epoch 20 , step 700 : 0.407189\n",
            "loss in epoch 20 , step 720 : 0.327979\n",
            "loss in epoch 20 , step 740 : 0.219249\n",
            "loss in epoch 20 , step 760 : 0.344839\n",
            "loss in epoch 20 , step 780 : 0.294894\n",
            "Accuracy in epoch 20 : 86.910004\n",
            "loss in epoch 21 , step 0 : 0.397754\n",
            "loss in epoch 21 , step 20 : 0.207162\n",
            "loss in epoch 21 , step 40 : 0.242528\n",
            "loss in epoch 21 , step 60 : 0.274507\n",
            "loss in epoch 21 , step 80 : 0.163018\n",
            "loss in epoch 21 , step 100 : 0.147478\n",
            "loss in epoch 21 , step 120 : 0.345167\n",
            "loss in epoch 21 , step 140 : 0.248414\n",
            "loss in epoch 21 , step 160 : 0.346079\n",
            "loss in epoch 21 , step 180 : 0.215630\n",
            "loss in epoch 21 , step 200 : 0.232437\n",
            "loss in epoch 21 , step 220 : 0.307041\n",
            "loss in epoch 21 , step 240 : 0.269526\n",
            "loss in epoch 21 , step 260 : 0.186187\n",
            "loss in epoch 21 , step 280 : 0.284808\n",
            "loss in epoch 21 , step 300 : 0.216029\n",
            "loss in epoch 21 , step 320 : 0.404174\n",
            "loss in epoch 21 , step 340 : 0.262397\n",
            "loss in epoch 21 , step 360 : 0.267661\n",
            "loss in epoch 21 , step 380 : 0.183908\n",
            "loss in epoch 21 , step 400 : 0.367428\n",
            "loss in epoch 21 , step 420 : 0.197039\n",
            "loss in epoch 21 , step 440 : 0.221628\n",
            "loss in epoch 21 , step 460 : 0.376488\n",
            "loss in epoch 21 , step 480 : 0.261912\n",
            "loss in epoch 21 , step 500 : 0.267832\n",
            "loss in epoch 21 , step 520 : 0.476286\n",
            "loss in epoch 21 , step 540 : 0.330725\n",
            "loss in epoch 21 , step 560 : 0.273457\n",
            "loss in epoch 21 , step 580 : 0.324653\n",
            "loss in epoch 21 , step 600 : 0.348421\n",
            "loss in epoch 21 , step 620 : 0.204730\n",
            "loss in epoch 21 , step 640 : 0.496193\n",
            "loss in epoch 21 , step 660 : 0.334406\n",
            "loss in epoch 21 , step 680 : 0.312471\n",
            "loss in epoch 21 , step 700 : 0.247854\n",
            "loss in epoch 21 , step 720 : 0.265744\n",
            "loss in epoch 21 , step 740 : 0.382163\n",
            "loss in epoch 21 , step 760 : 0.372106\n",
            "loss in epoch 21 , step 780 : 0.525189\n",
            "Accuracy in epoch 21 : 87.480003\n",
            "loss in epoch 22 , step 0 : 0.233906\n",
            "loss in epoch 22 , step 20 : 0.388364\n",
            "loss in epoch 22 , step 40 : 0.225983\n",
            "loss in epoch 22 , step 60 : 0.246046\n",
            "loss in epoch 22 , step 80 : 0.358737\n",
            "loss in epoch 22 , step 100 : 0.226207\n",
            "loss in epoch 22 , step 120 : 0.277963\n",
            "loss in epoch 22 , step 140 : 0.244320\n",
            "loss in epoch 22 , step 160 : 0.200837\n",
            "loss in epoch 22 , step 180 : 0.253357\n",
            "loss in epoch 22 , step 200 : 0.276466\n",
            "loss in epoch 22 , step 220 : 0.392731\n",
            "loss in epoch 22 , step 240 : 0.383263\n",
            "loss in epoch 22 , step 260 : 0.234525\n",
            "loss in epoch 22 , step 280 : 0.206477\n",
            "loss in epoch 22 , step 300 : 0.248637\n",
            "loss in epoch 22 , step 320 : 0.275243\n",
            "loss in epoch 22 , step 340 : 0.415471\n",
            "loss in epoch 22 , step 360 : 0.272180\n",
            "loss in epoch 22 , step 380 : 0.373306\n",
            "loss in epoch 22 , step 400 : 0.325311\n",
            "loss in epoch 22 , step 420 : 0.338310\n",
            "loss in epoch 22 , step 440 : 0.313274\n",
            "loss in epoch 22 , step 460 : 0.475016\n",
            "loss in epoch 22 , step 480 : 0.161830\n",
            "loss in epoch 22 , step 500 : 0.179445\n",
            "loss in epoch 22 , step 520 : 0.198899\n",
            "loss in epoch 22 , step 540 : 0.312025\n",
            "loss in epoch 22 , step 560 : 0.295296\n",
            "loss in epoch 22 , step 580 : 0.320081\n",
            "loss in epoch 22 , step 600 : 0.221167\n",
            "loss in epoch 22 , step 620 : 0.262132\n",
            "loss in epoch 22 , step 640 : 0.387934\n",
            "loss in epoch 22 , step 660 : 0.268124\n",
            "loss in epoch 22 , step 680 : 0.236427\n",
            "loss in epoch 22 , step 700 : 0.200465\n",
            "loss in epoch 22 , step 720 : 0.196332\n",
            "loss in epoch 22 , step 740 : 0.262530\n",
            "loss in epoch 22 , step 760 : 0.367048\n",
            "loss in epoch 22 , step 780 : 0.297139\n",
            "Accuracy in epoch 22 : 83.860001\n",
            "loss in epoch 23 , step 0 : 0.105845\n",
            "loss in epoch 23 , step 20 : 0.128694\n",
            "loss in epoch 23 , step 40 : 0.199894\n",
            "loss in epoch 23 , step 60 : 0.348387\n",
            "loss in epoch 23 , step 80 : 0.344619\n",
            "loss in epoch 23 , step 100 : 0.319277\n",
            "loss in epoch 23 , step 120 : 0.439790\n",
            "loss in epoch 23 , step 140 : 0.541697\n",
            "loss in epoch 23 , step 160 : 0.255230\n",
            "loss in epoch 23 , step 180 : 0.194003\n",
            "loss in epoch 23 , step 200 : 0.301234\n",
            "loss in epoch 23 , step 220 : 0.312509\n",
            "loss in epoch 23 , step 240 : 0.319754\n",
            "loss in epoch 23 , step 260 : 0.348860\n",
            "loss in epoch 23 , step 280 : 0.231050\n",
            "loss in epoch 23 , step 300 : 0.326103\n",
            "loss in epoch 23 , step 320 : 0.403938\n",
            "loss in epoch 23 , step 340 : 0.379122\n",
            "loss in epoch 23 , step 360 : 0.277547\n",
            "loss in epoch 23 , step 380 : 0.229011\n",
            "loss in epoch 23 , step 400 : 0.214245\n",
            "loss in epoch 23 , step 420 : 0.296170\n",
            "loss in epoch 23 , step 440 : 0.236064\n",
            "loss in epoch 23 , step 460 : 0.219267\n",
            "loss in epoch 23 , step 480 : 0.274858\n",
            "loss in epoch 23 , step 500 : 0.286479\n",
            "loss in epoch 23 , step 520 : 0.292307\n",
            "loss in epoch 23 , step 540 : 0.243431\n",
            "loss in epoch 23 , step 560 : 0.156269\n",
            "loss in epoch 23 , step 580 : 0.328112\n",
            "loss in epoch 23 , step 600 : 0.417110\n",
            "loss in epoch 23 , step 620 : 0.354251\n",
            "loss in epoch 23 , step 640 : 0.323284\n",
            "loss in epoch 23 , step 660 : 0.308671\n",
            "loss in epoch 23 , step 680 : 0.304047\n",
            "loss in epoch 23 , step 700 : 0.291204\n",
            "loss in epoch 23 , step 720 : 0.421643\n",
            "loss in epoch 23 , step 740 : 0.355571\n",
            "loss in epoch 23 , step 760 : 0.276040\n",
            "loss in epoch 23 , step 780 : 0.211481\n",
            "Accuracy in epoch 23 : 87.570000\n",
            "loss in epoch 24 , step 0 : 0.170002\n",
            "loss in epoch 24 , step 20 : 0.286991\n",
            "loss in epoch 24 , step 40 : 0.294533\n",
            "loss in epoch 24 , step 60 : 0.259419\n",
            "loss in epoch 24 , step 80 : 0.223637\n",
            "loss in epoch 24 , step 100 : 0.199827\n",
            "loss in epoch 24 , step 120 : 0.300376\n",
            "loss in epoch 24 , step 140 : 0.182232\n",
            "loss in epoch 24 , step 160 : 0.408025\n",
            "loss in epoch 24 , step 180 : 0.127164\n",
            "loss in epoch 24 , step 200 : 0.363179\n",
            "loss in epoch 24 , step 220 : 0.226341\n",
            "loss in epoch 24 , step 240 : 0.308270\n",
            "loss in epoch 24 , step 260 : 0.233274\n",
            "loss in epoch 24 , step 280 : 0.193591\n",
            "loss in epoch 24 , step 300 : 0.346182\n",
            "loss in epoch 24 , step 320 : 0.334410\n",
            "loss in epoch 24 , step 340 : 0.255143\n",
            "loss in epoch 24 , step 360 : 0.313929\n",
            "loss in epoch 24 , step 380 : 0.216131\n",
            "loss in epoch 24 , step 400 : 0.257968\n",
            "loss in epoch 24 , step 420 : 0.322248\n",
            "loss in epoch 24 , step 440 : 0.307771\n",
            "loss in epoch 24 , step 460 : 0.611971\n",
            "loss in epoch 24 , step 480 : 0.218401\n",
            "loss in epoch 24 , step 500 : 0.388456\n",
            "loss in epoch 24 , step 520 : 0.384278\n",
            "loss in epoch 24 , step 540 : 0.271831\n",
            "loss in epoch 24 , step 560 : 0.431142\n",
            "loss in epoch 24 , step 580 : 0.094233\n",
            "loss in epoch 24 , step 600 : 0.304567\n",
            "loss in epoch 24 , step 620 : 0.450464\n",
            "loss in epoch 24 , step 640 : 0.277629\n",
            "loss in epoch 24 , step 660 : 0.235665\n",
            "loss in epoch 24 , step 680 : 0.329230\n",
            "loss in epoch 24 , step 700 : 0.315952\n",
            "loss in epoch 24 , step 720 : 0.270383\n",
            "loss in epoch 24 , step 740 : 0.428610\n",
            "loss in epoch 24 , step 760 : 0.353122\n",
            "loss in epoch 24 , step 780 : 0.301843\n",
            "Accuracy in epoch 24 : 85.910004\n",
            "loss in epoch 25 , step 0 : 0.259355\n",
            "loss in epoch 25 , step 20 : 0.291155\n",
            "loss in epoch 25 , step 40 : 0.416181\n",
            "loss in epoch 25 , step 60 : 0.247161\n",
            "loss in epoch 25 , step 80 : 0.116530\n",
            "loss in epoch 25 , step 100 : 0.170121\n",
            "loss in epoch 25 , step 120 : 0.185632\n",
            "loss in epoch 25 , step 140 : 0.441369\n",
            "loss in epoch 25 , step 160 : 0.376022\n",
            "loss in epoch 25 , step 180 : 0.135705\n",
            "loss in epoch 25 , step 200 : 0.262045\n",
            "loss in epoch 25 , step 220 : 0.337192\n",
            "loss in epoch 25 , step 240 : 0.335489\n",
            "loss in epoch 25 , step 260 : 0.220545\n",
            "loss in epoch 25 , step 280 : 0.266532\n",
            "loss in epoch 25 , step 300 : 0.358179\n",
            "loss in epoch 25 , step 320 : 0.263778\n",
            "loss in epoch 25 , step 340 : 0.354571\n",
            "loss in epoch 25 , step 360 : 0.241166\n",
            "loss in epoch 25 , step 380 : 0.267688\n",
            "loss in epoch 25 , step 400 : 0.330762\n",
            "loss in epoch 25 , step 420 : 0.299245\n",
            "loss in epoch 25 , step 440 : 0.328755\n",
            "loss in epoch 25 , step 460 : 0.330499\n",
            "loss in epoch 25 , step 480 : 0.167867\n",
            "loss in epoch 25 , step 500 : 0.195827\n",
            "loss in epoch 25 , step 520 : 0.657782\n",
            "loss in epoch 25 , step 540 : 0.349676\n",
            "loss in epoch 25 , step 560 : 0.324730\n",
            "loss in epoch 25 , step 580 : 0.286266\n",
            "loss in epoch 25 , step 600 : 0.174067\n",
            "loss in epoch 25 , step 620 : 0.226577\n",
            "loss in epoch 25 , step 640 : 0.226053\n",
            "loss in epoch 25 , step 660 : 0.171601\n",
            "loss in epoch 25 , step 680 : 0.230807\n",
            "loss in epoch 25 , step 700 : 0.155490\n",
            "loss in epoch 25 , step 720 : 0.303736\n",
            "loss in epoch 25 , step 740 : 0.510579\n",
            "loss in epoch 25 , step 760 : 0.233019\n",
            "loss in epoch 25 , step 780 : 0.398158\n",
            "Accuracy in epoch 25 : 86.760002\n",
            "loss in epoch 26 , step 0 : 0.283616\n",
            "loss in epoch 26 , step 20 : 0.279176\n",
            "loss in epoch 26 , step 40 : 0.284332\n",
            "loss in epoch 26 , step 60 : 0.347892\n",
            "loss in epoch 26 , step 80 : 0.369243\n",
            "loss in epoch 26 , step 100 : 0.372971\n",
            "loss in epoch 26 , step 120 : 0.181768\n",
            "loss in epoch 26 , step 140 : 0.165556\n",
            "loss in epoch 26 , step 160 : 0.140131\n",
            "loss in epoch 26 , step 180 : 0.365744\n",
            "loss in epoch 26 , step 200 : 0.211951\n",
            "loss in epoch 26 , step 220 : 0.392944\n",
            "loss in epoch 26 , step 240 : 0.369942\n",
            "loss in epoch 26 , step 260 : 0.154467\n",
            "loss in epoch 26 , step 280 : 0.224608\n",
            "loss in epoch 26 , step 300 : 0.309589\n",
            "loss in epoch 26 , step 320 : 0.273499\n",
            "loss in epoch 26 , step 340 : 0.121153\n",
            "loss in epoch 26 , step 360 : 0.342020\n",
            "loss in epoch 26 , step 380 : 0.157780\n",
            "loss in epoch 26 , step 400 : 0.126425\n",
            "loss in epoch 26 , step 420 : 0.399451\n",
            "loss in epoch 26 , step 440 : 0.166213\n",
            "loss in epoch 26 , step 460 : 0.201788\n",
            "loss in epoch 26 , step 480 : 0.268865\n",
            "loss in epoch 26 , step 500 : 0.250518\n",
            "loss in epoch 26 , step 520 : 0.316204\n",
            "loss in epoch 26 , step 540 : 0.176869\n",
            "loss in epoch 26 , step 560 : 0.221885\n",
            "loss in epoch 26 , step 580 : 0.236115\n",
            "loss in epoch 26 , step 600 : 0.275480\n",
            "loss in epoch 26 , step 620 : 0.354330\n",
            "loss in epoch 26 , step 640 : 0.200935\n",
            "loss in epoch 26 , step 660 : 0.377561\n",
            "loss in epoch 26 , step 680 : 0.394372\n",
            "loss in epoch 26 , step 700 : 0.319938\n",
            "loss in epoch 26 , step 720 : 0.356480\n",
            "loss in epoch 26 , step 740 : 0.290051\n",
            "loss in epoch 26 , step 760 : 0.193379\n",
            "loss in epoch 26 , step 780 : 0.298738\n",
            "Accuracy in epoch 26 : 86.570000\n",
            "loss in epoch 27 , step 0 : 0.198088\n",
            "loss in epoch 27 , step 20 : 0.322335\n",
            "loss in epoch 27 , step 40 : 0.345788\n",
            "loss in epoch 27 , step 60 : 0.331549\n",
            "loss in epoch 27 , step 80 : 0.342414\n",
            "loss in epoch 27 , step 100 : 0.162043\n",
            "loss in epoch 27 , step 120 : 0.174041\n",
            "loss in epoch 27 , step 140 : 0.461179\n",
            "loss in epoch 27 , step 160 : 0.123590\n",
            "loss in epoch 27 , step 180 : 0.109524\n",
            "loss in epoch 27 , step 200 : 0.284293\n",
            "loss in epoch 27 , step 220 : 0.339457\n",
            "loss in epoch 27 , step 240 : 0.304466\n",
            "loss in epoch 27 , step 260 : 0.179670\n",
            "loss in epoch 27 , step 280 : 0.492051\n",
            "loss in epoch 27 , step 300 : 0.166627\n",
            "loss in epoch 27 , step 320 : 0.342189\n",
            "loss in epoch 27 , step 340 : 0.278871\n",
            "loss in epoch 27 , step 360 : 0.168918\n",
            "loss in epoch 27 , step 380 : 0.188050\n",
            "loss in epoch 27 , step 400 : 0.259395\n",
            "loss in epoch 27 , step 420 : 0.300203\n",
            "loss in epoch 27 , step 440 : 0.288299\n",
            "loss in epoch 27 , step 460 : 0.133881\n",
            "loss in epoch 27 , step 480 : 0.320001\n",
            "loss in epoch 27 , step 500 : 0.196863\n",
            "loss in epoch 27 , step 520 : 0.388259\n",
            "loss in epoch 27 , step 540 : 0.347989\n",
            "loss in epoch 27 , step 560 : 0.268030\n",
            "loss in epoch 27 , step 580 : 0.106382\n",
            "loss in epoch 27 , step 600 : 0.175101\n",
            "loss in epoch 27 , step 620 : 0.198116\n",
            "loss in epoch 27 , step 640 : 0.169329\n",
            "loss in epoch 27 , step 660 : 0.200348\n",
            "loss in epoch 27 , step 680 : 0.337817\n",
            "loss in epoch 27 , step 700 : 0.387621\n",
            "loss in epoch 27 , step 720 : 0.501123\n",
            "loss in epoch 27 , step 740 : 0.138557\n",
            "loss in epoch 27 , step 760 : 0.368513\n",
            "loss in epoch 27 , step 780 : 0.214591\n",
            "Accuracy in epoch 27 : 80.269997\n",
            "loss in epoch 28 , step 0 : 0.153699\n",
            "loss in epoch 28 , step 20 : 0.287062\n",
            "loss in epoch 28 , step 40 : 0.300187\n",
            "loss in epoch 28 , step 60 : 0.317815\n",
            "loss in epoch 28 , step 80 : 0.299259\n",
            "loss in epoch 28 , step 100 : 0.232658\n",
            "loss in epoch 28 , step 120 : 0.131479\n",
            "loss in epoch 28 , step 140 : 0.207022\n",
            "loss in epoch 28 , step 160 : 0.201518\n",
            "loss in epoch 28 , step 180 : 0.272947\n",
            "loss in epoch 28 , step 200 : 0.174339\n",
            "loss in epoch 28 , step 220 : 0.204254\n",
            "loss in epoch 28 , step 240 : 0.142281\n",
            "loss in epoch 28 , step 260 : 0.239859\n",
            "loss in epoch 28 , step 280 : 0.213376\n",
            "loss in epoch 28 , step 300 : 0.134880\n",
            "loss in epoch 28 , step 320 : 0.200934\n",
            "loss in epoch 28 , step 340 : 0.262656\n",
            "loss in epoch 28 , step 360 : 0.533257\n",
            "loss in epoch 28 , step 380 : 0.123682\n",
            "loss in epoch 28 , step 400 : 0.279438\n",
            "loss in epoch 28 , step 420 : 0.199895\n",
            "loss in epoch 28 , step 440 : 0.303649\n",
            "loss in epoch 28 , step 460 : 0.228124\n",
            "loss in epoch 28 , step 480 : 0.430947\n",
            "loss in epoch 28 , step 500 : 0.167508\n",
            "loss in epoch 28 , step 520 : 0.442678\n",
            "loss in epoch 28 , step 540 : 0.262940\n",
            "loss in epoch 28 , step 560 : 0.395518\n",
            "loss in epoch 28 , step 580 : 0.321592\n",
            "loss in epoch 28 , step 600 : 0.368001\n",
            "loss in epoch 28 , step 620 : 0.163287\n",
            "loss in epoch 28 , step 640 : 0.495246\n",
            "loss in epoch 28 , step 660 : 0.331634\n",
            "loss in epoch 28 , step 680 : 0.281607\n",
            "loss in epoch 28 , step 700 : 0.302461\n",
            "loss in epoch 28 , step 720 : 0.189756\n",
            "loss in epoch 28 , step 740 : 0.198344\n",
            "loss in epoch 28 , step 760 : 0.250229\n",
            "loss in epoch 28 , step 780 : 0.228455\n",
            "Accuracy in epoch 28 : 87.870003\n",
            "loss in epoch 29 , step 0 : 0.233368\n",
            "loss in epoch 29 , step 20 : 0.242518\n",
            "loss in epoch 29 , step 40 : 0.172949\n",
            "loss in epoch 29 , step 60 : 0.473441\n",
            "loss in epoch 29 , step 80 : 0.318135\n",
            "loss in epoch 29 , step 100 : 0.284925\n",
            "loss in epoch 29 , step 120 : 0.338809\n",
            "loss in epoch 29 , step 140 : 0.283041\n",
            "loss in epoch 29 , step 160 : 0.225863\n",
            "loss in epoch 29 , step 180 : 0.316547\n",
            "loss in epoch 29 , step 200 : 0.232930\n",
            "loss in epoch 29 , step 220 : 0.394596\n",
            "loss in epoch 29 , step 240 : 0.086791\n",
            "loss in epoch 29 , step 260 : 0.460861\n",
            "loss in epoch 29 , step 280 : 0.413671\n",
            "loss in epoch 29 , step 300 : 0.372626\n",
            "loss in epoch 29 , step 320 : 0.195610\n",
            "loss in epoch 29 , step 340 : 0.197022\n",
            "loss in epoch 29 , step 360 : 0.292527\n",
            "loss in epoch 29 , step 380 : 0.202132\n",
            "loss in epoch 29 , step 400 : 0.236113\n",
            "loss in epoch 29 , step 420 : 0.286975\n",
            "loss in epoch 29 , step 440 : 0.243162\n",
            "loss in epoch 29 , step 460 : 0.260289\n",
            "loss in epoch 29 , step 480 : 0.348031\n",
            "loss in epoch 29 , step 500 : 0.245207\n",
            "loss in epoch 29 , step 520 : 0.134067\n",
            "loss in epoch 29 , step 540 : 0.405069\n",
            "loss in epoch 29 , step 560 : 0.188515\n",
            "loss in epoch 29 , step 580 : 0.288862\n",
            "loss in epoch 29 , step 600 : 0.177900\n",
            "loss in epoch 29 , step 620 : 0.162289\n",
            "loss in epoch 29 , step 640 : 0.391195\n",
            "loss in epoch 29 , step 660 : 0.401852\n",
            "loss in epoch 29 , step 680 : 0.350122\n",
            "loss in epoch 29 , step 700 : 0.210096\n",
            "loss in epoch 29 , step 720 : 0.279945\n",
            "loss in epoch 29 , step 740 : 0.297862\n",
            "loss in epoch 29 , step 760 : 0.208640\n",
            "loss in epoch 29 , step 780 : 0.381383\n",
            "Accuracy in epoch 29 : 85.230003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_KUubnOUrQ6C"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}